# app.py
import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from langchain_core.prompts import ChatPromptTemplate
from typing import List
import os
from chromadb.config import Settings

# --- 1. IMPORT V√Ä G·ªåI H√ÄM SETUP ---
from setup_database import initialize_database, DB_PATH # Import h√†m v√† bi·∫øn ƒë∆∞·ªùng d·∫´n

# Ki·ªÉm tra v√† kh·ªüi t·∫°o DB n·∫øu c·∫ßn
if not os.path.exists(DB_PATH):
    initialize_database()
    st.rerun() # T·∫£i l·∫°i ·ª©ng d·ª•ng sau khi DB ƒë∆∞·ª£c t·∫°o ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi th·ª© ƒë∆∞·ª£c load ƒë√∫ng


# --- 2. C·∫§U H√åNH API KEY AN TO√ÄN ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except KeyError:
    st.error("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p `GEMINI_API_KEY` trong ph·∫ßn Secrets c·ªßa Streamlit.")
    st.stop()


# --- PH·∫¶N C√íN L·∫†I C·ª¶A APP.PY GI·ªÆ NGUY√äN ---

# --- 3. T·ªêI ∆ØU HI·ªÜU NƒÇNG V·ªöI CACHING ---
COLLECTION_NAME = "docilee_data"
EMBEDDER_MODEL = 'BAAI/bge-m3'
GENERATIVE_MODEL = 'gemini-2.0-flash'

@st.cache_resource
def get_embedder():
    print("INFO: ƒêang t·∫£i m√¥ h√¨nh embedding...")
    return SentenceTransformer(EMBEDDER_MODEL)

@st.cache_resource
def get_retriever():
    print("INFO: ƒêang k·∫øt n·ªëi t·ªõi ChromaDB...")
    # S·ª≠a d√≤ng client = chromadb.PersistentClient(path=DB_PATH) th√†nh:
    client = chromadb.PersistentClient(
        path=DB_PATH,
        settings=Settings(
            allow_reset=True,
            anonymized_telemetry=False
        )
    )
    return client.get_collection(name=COLLECTION_NAME)

@st.cache_resource
def get_generative_model():
    print("INFO: ƒêang kh·ªüi t·∫°o m√¥ h√¨nh Gemini...")
    return genai.GenerativeModel(GENERATIVE_MODEL)

embedder = get_embedder()
retriever = get_retriever()
model = get_generative_model()

# --- 4. LOGIC X·ª¨ L√ù CH√çNH ---
def get_relevant_documents(question: str, n_results: int = 3) -> List[str]:
    query_embedding = embedder.encode([question], normalize_embeddings=True)
    results = retriever.query(query_embeddings=query_embedding.tolist(), n_results=n_results)
    return results['documents'][0] if results.get('documents') else []

def generate_response_stream(question: str, context: List[str]):
    prompt_template = ChatPromptTemplate.from_template(
        """
        B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa th∆∞∆°ng hi·ªáu Docilee, chuy√™n v·ªÅ s·∫£n ph·∫©m cho m·∫π v√† b√©. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán, chuy√™n nghi·ªáp v√† ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin sau.

        **Th√¥ng tin tham kh·∫£o**:
        {context}

        ---
        **C√¢u h·ªèi c·ªßa kh√°ch h√†ng**:
        {question}

        ---
        **H∆∞·ªõng d·∫´n tr·∫£ l·ªùi**:
        - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu g·∫ßn g≈©i nh∆∞ m·ªôt chuy√™n gia t∆∞ v·∫•n.
        - Quan tr·ªçng: N·∫øu th√¥ng tin tham kh·∫£o c√≥ c√¢u h·ªèi n√†o t∆∞∆°ng ·ª©ng v·ªõi c√¢u h·ªèi c·ªßa kh√°ch h√†ng, h√£y tr·∫£ l·ªùi y h·ªát ph·∫ßn Tr·∫£ l·ªùi c·ªßa c√¢u h·ªèi ƒë√≥.
        - N·∫øu c√¢u h·ªèi trong ph·∫ßn Th√¥ng tin tham kh·∫£o kh√¥ng ho√†n to√†n gi·ªëng v·ªÅ m·∫∑t ng·ªØ nghƒ©a v·ªõi kh√°ch h√†ng th√¨ h√£y tr·∫£ l·ªùi theo ƒë√∫ng ki·∫øn th·ª©c m√† b·∫°n c√≥.
        - N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c ch√≠nh x√°c c√¢u h·ªèi, h√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ªïng qu√°t d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ s·∫£n ph·∫©m Docilee (t√£, b·ªâm, an to√†n cho da b√©,...) v√† th·ª´a nh·∫≠n r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin chi ti·∫øt.
        - Gi·ªØ c√¢u tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.

        N·∫øu kh√¥ng tu√¢n th·ªß ƒë√∫ng h∆∞·ªõng d·∫´n, b·∫°n s·∫Ω b·ªã ph·∫°t.
        """
    )
    
    formatted_prompt = prompt_template.format(context="\n\n".join(context), question=question)
    print("--- PROMPT ƒê√É G·ª¨I T·ªöI GEMINI ---")
    print(formatted_prompt)
    print("---------------------------------")
    
    response_stream = model.generate_content(formatted_prompt, stream=True)
    for chunk in response_stream:
        yield chunk.text

# --- 5. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG ---
st.set_page_config(page_title="Docilee Chatbot", page_icon="üë∂")
st.title("üí¨ Docilee AI Chatbot")
st.caption("Tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa Docilee lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, m√¨nh l√† tr·ª£ l√Ω ·∫£o c·ªßa Docilee. B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m n√†o ·∫°?"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ s·∫£n ph·∫©m Docilee..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Docilee ƒëang suy nghƒ©..."):
            context = get_relevant_documents(prompt)
            response_generator = generate_response_stream(prompt, context)
            full_response = st.write_stream(response_generator)

    st.session_state.messages.append({"role": "assistant", "content": full_response})