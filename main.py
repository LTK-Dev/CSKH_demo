# # app.py
# import streamlit as st
# import chromadb
# from sentence_transformers import SentenceTransformer
# import google.generativeai as genai
# from langchain_core.prompts import ChatPromptTemplate
# from typing import List
# import os
# from chromadb.config import Settings

# # --- 1. IMPORT V√Ä G·ªåI H√ÄM SETUP ---
# from setup_database import initialize_database, DB_PATH # Import h√†m v√† bi·∫øn ƒë∆∞·ªùng d·∫´n

# # Ki·ªÉm tra v√† kh·ªüi t·∫°o DB n·∫øu c·∫ßn
# if not os.path.exists(DB_PATH):
#     initialize_database()
#     st.rerun() # T·∫£i l·∫°i ·ª©ng d·ª•ng sau khi DB ƒë∆∞·ª£c t·∫°o ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi th·ª© ƒë∆∞·ª£c load ƒë√∫ng


# # --- 2. C·∫§U H√åNH API KEY AN TO√ÄN ---
# try:
#     GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
#     genai.configure(api_key=GEMINI_API_KEY)
# except KeyError:
#     st.error("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p `GEMINI_API_KEY` trong ph·∫ßn Secrets c·ªßa Streamlit.")
#     st.stop()


# # --- PH·∫¶N C√íN L·∫†I C·ª¶A APP.PY GI·ªÆ NGUY√äN ---

# # --- 3. T·ªêI ∆ØU HI·ªÜU NƒÇNG V·ªöI CACHING ---
# COLLECTION_NAME = "docilee_data"
# EMBEDDER_MODEL = 'BAAI/bge-m3'
# GENERATIVE_MODEL = 'gemini-2.0-flash'

# @st.cache_resource
# def get_embedder():
#     print("INFO: ƒêang t·∫£i m√¥ h√¨nh embedding...")
#     return SentenceTransformer(EMBEDDER_MODEL)

# @st.cache_resource
# def get_retriever():
#     print("INFO: ƒêang k·∫øt n·ªëi t·ªõi ChromaDB...")
#     # S·ª≠a d√≤ng client = chromadb.PersistentClient(path=DB_PATH) th√†nh:
#     client = chromadb.PersistentClient(
#         path=DB_PATH,
#         settings=Settings(
#             allow_reset=True,
#             anonymized_telemetry=False
#         )
#     )
#     return client.get_collection(name=COLLECTION_NAME)

# @st.cache_resource
# def get_generative_model():
#     print("INFO: ƒêang kh·ªüi t·∫°o m√¥ h√¨nh Gemini...")
#     return genai.GenerativeModel(GENERATIVE_MODEL)

# embedder = get_embedder()
# retriever = get_retriever()
# model = get_generative_model()

# # --- 4. LOGIC X·ª¨ L√ù CH√çNH ---
# def get_relevant_documents(question: str, n_results: int = 3) -> List[str]:
#     query_embedding = embedder.encode([question], normalize_embeddings=True)
#     results = retriever.query(query_embeddings=query_embedding.tolist(), n_results=n_results)
#     return results['documents'][0] if results.get('documents') else []

# def generate_response_stream(question: str, context: List[str]):
#     prompt_template = ChatPromptTemplate.from_template(
#         """
#         B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa th∆∞∆°ng hi·ªáu Docilee, chuy√™n v·ªÅ s·∫£n ph·∫©m cho m·∫π v√† b√©. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán, chuy√™n nghi·ªáp v√† ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin sau.

#         **Th√¥ng tin tham kh·∫£o**:
#         {context}

#         ---
#         **C√¢u h·ªèi c·ªßa kh√°ch h√†ng**:
#         {question}

#         ---
#         **H∆∞·ªõng d·∫´n tr·∫£ l·ªùi**:
#         - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu g·∫ßn g≈©i nh∆∞ m·ªôt chuy√™n gia t∆∞ v·∫•n.
#         - Quan tr·ªçng: N·∫øu th√¥ng tin tham kh·∫£o c√≥ c√¢u h·ªèi n√†o t∆∞∆°ng ·ª©ng v·ªõi c√¢u h·ªèi c·ªßa kh√°ch h√†ng, h√£y tr·∫£ l·ªùi y h·ªát ph·∫ßn Tr·∫£ l·ªùi c·ªßa c√¢u h·ªèi ƒë√≥.
#         - N·∫øu c√¢u h·ªèi trong ph·∫ßn Th√¥ng tin tham kh·∫£o kh√¥ng ho√†n to√†n gi·ªëng v·ªÅ m·∫∑t ng·ªØ nghƒ©a v·ªõi kh√°ch h√†ng th√¨ h√£y tr·∫£ l·ªùi theo ƒë√∫ng ki·∫øn th·ª©c m√† b·∫°n c√≥.
#         - N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c ch√≠nh x√°c c√¢u h·ªèi, h√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ªïng qu√°t d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ s·∫£n ph·∫©m Docilee (t√£, b·ªâm, an to√†n cho da b√©,...) v√† th·ª´a nh·∫≠n r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin chi ti·∫øt.
#         - Gi·ªØ c√¢u tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.

#         N·∫øu kh√¥ng tu√¢n th·ªß ƒë√∫ng h∆∞·ªõng d·∫´n, b·∫°n s·∫Ω b·ªã ph·∫°t.
#         """
#     )
    
#     formatted_prompt = prompt_template.format(context="\n\n".join(context), question=question)
#     print("--- PROMPT ƒê√É G·ª¨I T·ªöI GEMINI ---")
#     print(formatted_prompt)
#     print("---------------------------------")
    
#     response_stream = model.generate_content(formatted_prompt, stream=True)
#     for chunk in response_stream:
#         yield chunk.text

# # --- 5. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG ---
# st.set_page_config(page_title="Docilee Chatbot", page_icon="üë∂")
# st.title("üí¨ Docilee AI Chatbot")
# st.caption("Tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa Docilee lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!")

# if "messages" not in st.session_state:
#     st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, m√¨nh l√† tr·ª£ l√Ω ·∫£o c·ªßa Docilee. B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m n√†o ·∫°?"}]

# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ s·∫£n ph·∫©m Docilee..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         with st.spinner("Docilee ƒëang suy nghƒ©..."):
#             context = get_relevant_documents(prompt)
#             response_generator = generate_response_stream(prompt, context)
#             full_response = st.write_stream(response_generator)

#     st.session_state.messages.append({"role": "assistant", "content": full_response})

# app.py (Phi√™n b·∫£n cu·ªëi c√πng, ho√†n ch·ªânh s·ª≠ d·ª•ng FAISS)
import streamlit as st
import pandas as pd
import os
import google.generativeai as genai
from typing import List

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho FAISS v√† LangChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate

# --- C·∫•u h√¨nh ---
CSV_FILE = 'hahaha_output.csv'
FAISS_INDEX_PATH = "faiss_index" # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u ch·ªâ m·ª•c FAISS
EMBEDDER_MODEL = 'intfloat/multilingual-e5-base'
GENERATIVE_MODEL = 'gemini-2.0-flash'

# --- 1. T·ªêI ∆ØU HI·ªÜU NƒÇNG V·ªöI CACHING ---
@st.cache_resource
def get_embedder():
    """T·∫£i v√† cache m√¥ h√¨nh embedding."""
    print("INFO: ƒêang t·∫£i m√¥ h√¨nh embedding...")
    # D√πng wrapper c·ªßa LangChain ƒë·ªÉ t∆∞∆°ng th√≠ch t·ªët h∆°n
    # 'cpu' ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch tr√™n m√¥i tr∆∞·ªùng deploy
    return SentenceTransformerEmbeddings(
        model_name=EMBEDDER_MODEL,
        model_kwargs={'device': 'cpu'}
    )

@st.cache_resource
def load_or_create_faiss_index(_embedder):
    """
    T·∫£i ch·ªâ m·ª•c FAISS n·∫øu ƒë√£ t·ªìn t·∫°i, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi t·ª´ CSV.
    H√†m n√†y s·∫Ω ch·∫°y m·ªôt l·∫ßn khi app kh·ªüi ƒë·ªông.
    """
    # 'allow_dangerous_deserialization=True' l√† c·∫ßn thi·∫øt cho LangChain phi√™n b·∫£n m·ªõi
    if os.path.exists(FAISS_INDEX_PATH):
        print(f"INFO: ƒêang t·∫£i ch·ªâ m·ª•c FAISS t·ª´ '{FAISS_INDEX_PATH}'...")
        return FAISS.load_local(FAISS_INDEX_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("Ch∆∞a c√≥ ch·ªâ m·ª•c FAISS. B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o m·ªõi t·ª´ file CSV...")
    print("INFO: B·∫Øt ƒë·∫ßu t·∫°o ch·ªâ m·ª•c FAISS m·ªõi...")

    try:
        df = pd.read_csv(CSV_FILE, encoding='utf-8')
        # T·∫°o n·ªôi dung cho c√°c documents
        documents = [
            f"C√¢u h·ªèi: {row['C√¢u h·ªèi']}\nTr·∫£ l·ªùi: {row['Tr·∫£ l·ªùi']}"
            for _, row in df.iterrows() if pd.notna(row['C√¢u h·ªèi']) or pd.notna(row['Tr·∫£ l·ªùi'])
        ]
        if not documents:
            st.error("L·ªói: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu h·ª£p l·ªá trong file CSV.")
            st.stop()
        
        # T·∫°o ch·ªâ m·ª•c vector t·ª´ c√°c document v√† m√¥ h√¨nh embedding
        print("INFO: ƒêang t·∫°o embeddings v√† ch·ªâ m·ª•c FAISS...")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        
        # L∆∞u ch·ªâ m·ª•c ra file ƒë·ªÉ l·∫ßn sau t·∫£i l·∫°i cho nhanh
        vectorstore.save_local(FAISS_INDEX_PATH)
        print(" INFO: T·∫°o v√† l∆∞u ch·ªâ m·ª•c FAISS th√†nh c√¥ng.")
        st.success("T·∫°o ch·ªâ m·ª•c th√†nh c√¥ng! Chatbot ƒë√£ s·∫µn s√†ng.")
        st.rerun() # T·∫£i l·∫°i app sau khi t·∫°o xong
        return vectorstore

    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{CSV_FILE}'. Vui l√≤ng t·∫£i file n√†y l√™n GitHub.")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫°o ch·ªâ m·ª•c FAISS: {e}")
        st.stop()


# --- 2. C·∫§U H√åNH API KEY AN TO√ÄN ---
try:
    # L·∫•y API key t·ª´ Streamlit Secrets khi deploy
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
except KeyError:
    st.error("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p `GEMINI_API_KEY` trong ph·∫ßn Secrets c·ªßa Streamlit.")
    st.stop()


# --- T·∫£i t√†i nguy√™n v√† kh·ªüi t·∫°o ---
embedder = get_embedder()
vector_store = load_or_create_faiss_index(embedder)
model = genai.GenerativeModel(GENERATIVE_MODEL)

# --- 3. LOGIC X·ª¨ L√ù CH√çNH ---
def get_relevant_documents(question: str, n_results: int = 3) -> List[str]:
    """S·ª≠ d·ª•ng FAISS ƒë·ªÉ truy v·∫•n c√°c t√†i li·ªáu li√™n quan."""
    # similarity_search tr·∫£ v·ªÅ c√°c ƒë·ªëi t∆∞·ª£ng Document c·ªßa LangChain
    results = vector_store.similarity_search(question, k=n_results)
    # Tr√≠ch xu·∫•t n·ªôi dung text t·ª´ c√°c document
    return [doc.page_content for doc in results]

def generate_response_stream(question: str, context: List[str]):
    prompt_template = ChatPromptTemplate.from_template(
        """
        B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa th∆∞∆°ng hi·ªáu Docilee, chuy√™n v·ªÅ s·∫£n ph·∫©m cho m·∫π v√† b√©. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán, chuy√™n nghi·ªáp v√† ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin sau.

        **Th√¥ng tin tham kh·∫£o**:
        {context}

        ---
        **C√¢u h·ªèi c·ªßa kh√°ch h√†ng**:
        {question}

        ---
        **H∆∞·ªõng d·∫´n tr·∫£ l·ªùi**:
        - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu g·∫ßn g≈©i nh∆∞ m·ªôt chuy√™n gia t∆∞ v·∫•n.
        - Quan tr·ªçng: N·∫øu th√¥ng tin tham kh·∫£o c√≥ c√¢u h·ªèi n√†o t∆∞∆°ng ·ª©ng v·ªõi c√¢u h·ªèi c·ªßa kh√°ch h√†ng, h√£y tr·∫£ l·ªùi y h·ªát ph·∫ßn Tr·∫£ l·ªùi c·ªßa c√¢u h·ªèi ƒë√≥.
        - N·∫øu c√¢u h·ªèi trong ph·∫ßn Th√¥ng tin tham kh·∫£o kh√¥ng ho√†n to√†n gi·ªëng v·ªÅ m·∫∑t ng·ªØ nghƒ©a v·ªõi kh√°ch h√†ng th√¨ h√£y tr·∫£ l·ªùi theo ƒë√∫ng ki·∫øn th·ª©c m√† b·∫°n c√≥.
        - N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c ch√≠nh x√°c c√¢u h·ªèi, h√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ªïng qu√°t d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ s·∫£n ph·∫©m Docilee (t√£, b·ªâm, an to√†n cho da b√©,...) v√† th·ª´a nh·∫≠n r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin chi ti·∫øt.
        - Gi·ªØ c√¢u tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.

        N·∫øu kh√¥ng tu√¢n th·ªß ƒë√∫ng h∆∞·ªõng d·∫´n, b·∫°n s·∫Ω b·ªã ph·∫°t.
        """
    )
    
    formatted_prompt = prompt_template.format(context="\n\n".join(context), question=question)
    response_stream = model.generate_content(formatted_prompt, stream=True)
    for chunk in response_stream:
        yield chunk.text


# --- 4. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG ---
st.set_page_config(page_title="Docilee Chatbot", page_icon="üë∂")
st.title("üí¨ Docilee AI Chatbot")
st.caption("Tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa Docilee (N·ªÅn t·∫£ng: FAISS + Gemini)")

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, m√¨nh l√† tr·ª£ l√Ω ·∫£o c·ªßa Docilee. B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m n√†o ·∫°?"}]

# Hi·ªÉn th·ªã c√°c tin nh·∫Øn ƒë√£ c√≥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Khung nh·∫≠p li·ªáu cho ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ s·∫£n ph·∫©m Docilee..."):
    # Th√™m v√† hi·ªÉn th·ªã tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # T·∫°o v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi c·ªßa bot
    with st.chat_message("assistant"):
        with st.spinner("Docilee ƒëang suy nghƒ©..."):
            # 1. T√¨m ki·∫øm th√¥ng tin li√™n quan
            context = get_relevant_documents(prompt)
            
            # 2. T·∫°o v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi theo t·ª´ng ph·∫ßn (streaming)
            response_generator = generate_response_stream(prompt, context)
            full_response = st.write_stream(response_generator)

    # Th√™m c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh c·ªßa bot v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})