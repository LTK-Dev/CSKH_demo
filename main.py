import streamlit as st
import pandas as pd
import os
import google.generativeai as genai
from typing import List

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho FAISS v√† LangChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate

# --- C·∫•u h√¨nh ---
# <<< THAY ƒê·ªîI: Tr·ªè ƒë·∫øn file d·ªØ li·ªáu s·∫£n ph·∫©m EKS
CSV_FILE = 'EKS.csv'
FAISS_INDEX_PATH = "faiss_index_eks" # <<< THAY ƒê·ªîI: Th∆∞ m·ª•c l∆∞u index ri√™ng cho EKS
EMBEDDER_MODEL = 'intfloat/multilingual-e5-base'
# <<< THAY ƒê·ªîI: B·∫°n c√≥ th·ªÉ ch·ªçn model ph√π h·ª£p, v√≠ d·ª• 'gemini-1.5-flash'
GENERATIVE_MODEL = 'gemini-2.0-flash'

# --- 1. T·ªêI ∆ØU HI·ªÜU NƒÇNG V·ªöI CACHING ---
@st.cache_resource
def get_embedder():
    """T·∫£i v√† cache m√¥ h√¨nh embedding."""
    print("INFO: ƒêang t·∫£i m√¥ h√¨nh embedding...")
    return SentenceTransformerEmbeddings(
        model_name=EMBEDDER_MODEL,
        model_kwargs={'device': 'cpu'}
    )

@st.cache_resource
def load_or_create_faiss_index(_embedder):
    """
    T·∫£i ch·ªâ m·ª•c FAISS n·∫øu ƒë√£ t·ªìn t·∫°i, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi t·ª´ CSV
    s·ª≠ d·ª•ng chi·∫øn thu·∫≠t "Field Chunking".
    """
    if os.path.exists(FAISS_INDEX_PATH):
        print(f"INFO: ƒêang t·∫£i ch·ªâ m·ª•c FAISS t·ª´ '{FAISS_INDEX_PATH}'...")
        return FAISS.load_local(FAISS_INDEX_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("Ch∆∞a c√≥ ch·ªâ m·ª•c FAISS. B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o m·ªõi t·ª´ file CSV...")
    print("INFO: B·∫Øt ƒë·∫ßu t·∫°o ch·ªâ m·ª•c FAISS m·ªõi...")

    try:
        df = pd.read_csv(CSV_FILE, encoding='utf-8')
        df.columns = [col.strip() for col in df.columns] # L√†m s·∫°ch t√™n c·ªôt

        # <<< THAY ƒê·ªîI L·ªöN: √Åp d·ª•ng chi·∫øn thu·∫≠t "Field Chunking"
        documents = []
        # C√°c c·ªôt ch·ª©a th√¥ng tin quan tr·ªçng c·∫ßn chunk
        fields_to_chunk = [
            "C√îNG D·ª§NG", "TH√ÄNH PH·∫¶N", "CH·ªà ƒê·ªäNH",
            "CH·ªêNG CH·ªà ƒê·ªäNH", "C√ÅCH D√ôNG", "B·∫¢O QU·∫¢N", "L∆ØU √ù KHI S·ª¨ D·ª§NG"
        ]

        for _, row in df.iterrows():
            product_name = row.get("S·∫¢N PH·∫®M", "Kh√¥ng r√µ t√™n")
            for field in fields_to_chunk:
                # Ch·ªâ t·∫°o document n·∫øu tr∆∞·ªùng ƒë√≥ c√≥ d·ªØ li·ªáu
                if field in row and pd.notna(row[field]):
                    chunk_content = f"S·∫£n ph·∫©m: {product_name}\nTh√¥ng tin v·ªÅ '{field}': {row[field]}"
                    documents.append(chunk_content)

        if not documents:
            st.error("L·ªói: Kh√¥ng th·ªÉ t·∫°o ƒë∆∞·ª£c c√°c 'chunks' vƒÉn b·∫£n t·ª´ file CSV. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u tr√∫c file v√† t√™n c√°c c·ªôt.")
            st.stop()
        
        print(f"INFO: ƒê√£ t·∫°o ƒë∆∞·ª£c {len(documents)} chunks t·ª´ file CSV.")
        print("INFO: ƒêang t·∫°o embeddings v√† ch·ªâ m·ª•c FAISS...")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        
        vectorstore.save_local(FAISS_INDEX_PATH)
        print(f"INFO: T·∫°o v√† l∆∞u ch·ªâ m·ª•c FAISS v√†o '{FAISS_INDEX_PATH}' th√†nh c√¥ng.")
        st.success("T·∫°o ch·ªâ m·ª•c s·∫£n ph·∫©m th√†nh c√¥ng! Chatbot ƒë√£ s·∫µn s√†ng.")
        # Kh√¥ng c·∫ßn rerun v√¨ cache resource s·∫Ω x·ª≠ l√Ω vi·ªác load l·∫°i
        return vectorstore

    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{CSV_FILE}'. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫°o ch·ªâ m·ª•c FAISS: {e}")
        st.stop()

# --- 2. C·∫§U H√åNH API KEY AN TO√ÄN ---
try:
    # L·∫•y API key t·ª´ Streamlit Secrets khi deploy
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
except Exception:
    st.error("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p `GEMINI_API_KEY` trong ph·∫ßn Secrets c·ªßa Streamlit.")
    st.stop()

# --- T·∫£i t√†i nguy√™n v√† kh·ªüi t·∫°o ---
embedder = get_embedder()
vector_store = load_or_create_faiss_index(embedder)
model = genai.GenerativeModel(GENERATIVE_MODEL)

# --- 3. LOGIC X·ª¨ L√ù CH√çNH ---
def get_relevant_documents(question: str, n_results: int = 5) -> List[str]: # <<< THAY ƒê·ªîI: TƒÉng s·ªë l∆∞·ª£ng context
    """S·ª≠ d·ª•ng FAISS ƒë·ªÉ truy v·∫•n c√°c chunks t√†i li·ªáu li√™n quan."""
    results = vector_store.similarity_search(question, k=n_results)
    return [doc.page_content for doc in results]

def generate_response_stream(question: str, context: List[str]):
    # <<< THAY ƒê·ªîI: C·∫≠p nh·∫≠t prompt ƒë·ªÉ ph√π h·ª£p v·ªõi EKS v√† field chunking
    prompt_template = ChatPromptTemplate.from_template(
        """
        B·∫°n l√† tr·ª£ l√Ω ·∫£o chuy√™n nghi·ªáp c·ªßa nh√£n h√†ng m·ªπ ph·∫©m Ekseption (EKS). Nhi·ªám v·ª• c·ªßa b·∫°n l√† t∆∞ v·∫•n ch√≠nh x√°c v√† th√¢n thi·ªán cho kh√°ch h√†ng d·ª±a tr√™n th√¥ng tin s·∫£n ph·∫©m ƒë∆∞·ª£c cung c·∫•p.

        **Th√¥ng tin tham kh·∫£o t·ª´ c√°c s·∫£n ph·∫©m (ƒë√£ ƒë∆∞·ª£c chia nh·ªè theo t·ª´ng tr∆∞·ªùng th√¥ng tin)**:
        ---
        {context}
        ---

        **C√¢u h·ªèi c·ªßa kh√°ch h√†ng**:
        {question}

        ---
        **H∆∞·ªõng d·∫´n tr·∫£ l·ªùi**:
        1.  **X∆∞ng h√¥**: Lu√¥n x∆∞ng l√† "EKS" v√† g·ªçi kh√°ch h√†ng l√† "b·∫°n". Gi·ªçng ƒëi·ªáu chuy√™n gia, t·ª± tin nh∆∞ng g·∫ßn g≈©i.
        2.  **T·ªïng h·ª£p th√¥ng tin**: D·ª±a v√†o c√°c m·∫£nh th√¥ng tin trong ph·∫ßn "Th√¥ng tin tham kh·∫£o" ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß nh·∫•t. C√°c m·∫£nh th√¥ng tin n√†y c√≥ th·ªÉ ƒë·∫øn t·ª´ nhi·ªÅu s·∫£n ph·∫©m kh√°c nhau, h√£y ch·ªçn l·ªçc th√¥ng tin ƒë√∫ng v·ªõi s·∫£n ph·∫©m m√† kh√°ch h√†ng h·ªèi.
        3.  **Tr√≠ch d·∫´n s·∫£n ph·∫©m**: Khi tr·∫£ l·ªùi, h√£y n√™u r√µ th√¥ng tin ƒë√≥ thu·ªôc v·ªÅ s·∫£n ph·∫©m n√†o. V√≠ d·ª•: "ƒê·ªëi v·ªõi s·∫£n ph·∫©m [T√™n s·∫£n ph·∫©m], c√¥ng d·ª•ng c·ªßa n√≥ l√†..."
        4.  **N·∫øu kh√¥ng ch·∫Øc ch·∫Øn**: N·∫øu th√¥ng tin tham kh·∫£o kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y tr·∫£ l·ªùi r·∫±ng: "C·∫£m ∆°n c√¢u h·ªèi c·ªßa b·∫°n. EKS ch∆∞a c√≥ th√¥ng tin chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ n√†y trong d·ªØ li·ªáu. B·∫°n vui l√≤ng cung c·∫•p th√™m chi ti·∫øt ho·∫∑c li√™n h·ªá chuy√™n gia ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n s√¢u h∆°n nh√©." Tuy·ªát ƒë·ªëi kh√¥ng t·ª± b·ªãa ƒë·∫∑t th√¥ng tin.
        5.  **Ng√¥n ng·ªØ**: S·ª≠ d·ª•ng ti·∫øng Vi·ªát, tr√¨nh b√†y r√µ r√†ng, d√πng markdown (g·∫°ch ƒë·∫ßu d√≤ng, in ƒë·∫≠m) ƒë·ªÉ c√¢u tr·∫£ l·ªùi d·ªÖ ƒë·ªçc.
        """
    )
    
    formatted_prompt = prompt_template.format(context="\n\n".join(context), question=question)
    response_stream = model.generate_content(formatted_prompt, stream=True)
    for chunk in response_stream:
        yield chunk.text

# --- 4. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG ---
# <<< THAY ƒê·ªîI: C·∫≠p nh·∫≠t giao di·ªán cho EKS
st.set_page_config(page_title="EKS Chatbot", page_icon="‚ú®")
st.title("üí¨ Chatbot T∆∞ v·∫•n s·∫£n ph·∫©m EKS")
st.caption("Tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa Ekseption (N·ªÅn t·∫£ng: Field Chunking + FAISS + Gemini)")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, EKS c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay? H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ s·∫£n ph·∫©m nh√©!"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("H·ªèi v·ªÅ c√¥ng d·ª•ng, th√†nh ph·∫ßn, c√°ch d√πng..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("EKS ƒëang t√¨m ki·∫øm c√¢u tr·∫£ l·ªùi..."):
            context = get_relevant_documents(prompt)
            
            # (T√πy ch·ªçn) In context ra ƒë·ªÉ debug
            # with st.expander("Xem context ƒë∆∞·ª£c t√¨m th·∫•y"):
            #     st.write(context)

            response_generator = generate_response_stream(prompt, context)
            full_response = st.write_stream(response_generator)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
